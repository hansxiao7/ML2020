{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reptile_regression.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4LuzpEhIo7X"
      },
      "source": [
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "a_range = [0.1, 5]\r\n",
        "b_range = [0, 2*np.pi]\r\n",
        "\r\n",
        "n_task = 100\r\n",
        "\r\n",
        "a = np.random.uniform(low=a_range[0], high=a_range[1], size=n_task)\r\n",
        "b = np.random.uniform(low=b_range[0], high=b_range[1], size=n_task)\r\n",
        "\r\n",
        "x_range = [-5, 5]\r\n",
        "n_sample = 10\r\n",
        "\r\n",
        "x_data = []\r\n",
        "y_data = []\r\n",
        "for i in range(n_task):\r\n",
        "  x = np.random.uniform(low=x_range[0], high=x_range[1], size=n_sample)\r\n",
        "  y = a[i] * np.sin(x + b[i])\r\n",
        "  x = tf.reshape(x, [10, 1])\r\n",
        "  x_data.append(x)\r\n",
        "  y_data.append(y)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrUn_5cqNAqU"
      },
      "source": [
        "# Build the regression model\r\n",
        "model = tf.keras.Sequential([tf.keras.Input(shape=(1, )),\r\n",
        "                             tf.keras.layers.Dense(40, activation='relu'),\r\n",
        "                             tf.keras.layers.Dense(40, activation='relu'),\r\n",
        "                             tf.keras.layers.Dense(1)\r\n",
        "])\r\n",
        "\r\n",
        "\r\n",
        "model_optimizer = tf.optimizers.Adam(0.01)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7aOr_ZkRUBC"
      },
      "source": [
        "# Get training task and testing task\r\n",
        "n_train_task = 70\r\n",
        "n_test_task = 30\r\n",
        "\r\n",
        "x_train = x_data[:n_train_task]\r\n",
        "y_train = y_data[:n_train_task]\r\n",
        "\r\n",
        "x_test = x_data[n_train_task:]\r\n",
        "y_test = y_data[n_train_task:]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhpNp9lzTNUa",
        "outputId": "7fd4f5b6-aba6-4412-af75-4c1f1a14b37a"
      },
      "source": [
        "# Use reptile to train the model\r\n",
        "epochs = 100\r\n",
        "\r\n",
        "for i in range(epochs):\r\n",
        "  total_loss = 0\r\n",
        "  for j in range(n_train_task):\r\n",
        "\r\n",
        "    # Make a deep copy for the initial variables\r\n",
        "    meta_variables = model.trainable_variables.copy()\r\n",
        "    for k in range(len(meta_variables)):\r\n",
        "      meta_variables[k] = tf.constant(meta_variables[k])\r\n",
        "    #print(meta_variables[0])\r\n",
        "  \r\n",
        "    inner_optimizer = tf.optimizers.Adam(0.01)\r\n",
        "\r\n",
        "    # Update 3 times for the model to calculate gradients\r\n",
        "    for m in range(3):\r\n",
        "      with tf.GradientTape() as tape:\r\n",
        "        y_pred = model(x_train[j])\r\n",
        "        loss = tf.keras.losses.mean_squared_error(y_train[j], y_pred)\r\n",
        "\r\n",
        "      gradients = tape.gradient(loss, model.trainable_variables)\r\n",
        "      inner_optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n",
        "    \r\n",
        "    y_pred = model(x_train[j])\r\n",
        "    total_loss += tf.reduce_mean(tf.keras.losses.mean_squared_error(y_train[j], y_pred))\r\n",
        "    \r\n",
        "    reptile_gradients = [0] * len(meta_variables)\r\n",
        "    for n in range(len(meta_variables)):\r\n",
        "      reptile_gradients[n] = (model.trainable_variables[n] - meta_variables[n]) * 0.5\r\n",
        "    # Normalize graidents\r\n",
        "    for k in range(len(meta_variables)):\r\n",
        "      model.trainable_variables[k] = meta_variables[k]\r\n",
        "\r\n",
        "    model_optimizer.apply_gradients(zip(reptile_gradients, model.trainable_variables))\r\n",
        "\r\n",
        "  print('Currrent Epoch is '+str(i) + '; Total loss is ' + str(total_loss.numpy()/70))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Currrent Epoch is 0; Total loss is 3.8823229108537944\n",
            "Currrent Epoch is 1; Total loss is 3.8810594831194196\n",
            "Currrent Epoch is 2; Total loss is 3.8807909284319195\n",
            "Currrent Epoch is 3; Total loss is 3.8805289132254464\n",
            "Currrent Epoch is 4; Total loss is 3.8802690778459823\n",
            "Currrent Epoch is 5; Total loss is 3.8800105503627234\n",
            "Currrent Epoch is 6; Total loss is 3.879756382533482\n",
            "Currrent Epoch is 7; Total loss is 3.8795013427734375\n",
            "Currrent Epoch is 8; Total loss is 3.881233869280134\n",
            "Currrent Epoch is 9; Total loss is 3.881597900390625\n",
            "Currrent Epoch is 10; Total loss is 3.881352015904018\n",
            "Currrent Epoch is 11; Total loss is 3.88110831124442\n",
            "Currrent Epoch is 12; Total loss is 3.8808663504464285\n",
            "Currrent Epoch is 13; Total loss is 3.880625261579241\n",
            "Currrent Epoch is 14; Total loss is 3.8803859165736605\n",
            "Currrent Epoch is 15; Total loss is 3.8801483154296874\n",
            "Currrent Epoch is 16; Total loss is 3.879911586216518\n",
            "Currrent Epoch is 17; Total loss is 3.8796761648995535\n",
            "Currrent Epoch is 18; Total loss is 3.879441179547991\n",
            "Currrent Epoch is 19; Total loss is 3.8792088099888393\n",
            "Currrent Epoch is 20; Total loss is 3.878976876395089\n",
            "Currrent Epoch is 21; Total loss is 3.878745814732143\n",
            "Currrent Epoch is 22; Total loss is 3.8785164969308035\n",
            "Currrent Epoch is 23; Total loss is 3.878287615094866\n",
            "Currrent Epoch is 24; Total loss is 3.8780609130859376\n",
            "Currrent Epoch is 25; Total loss is 3.8778346470424108\n",
            "Currrent Epoch is 26; Total loss is 3.8776096888950895\n",
            "Currrent Epoch is 27; Total loss is 3.8773838588169642\n",
            "Currrent Epoch is 28; Total loss is 3.877161952427455\n",
            "Currrent Epoch is 29; Total loss is 3.8769400460379466\n",
            "Currrent Epoch is 30; Total loss is 3.8767181396484376\n",
            "Currrent Epoch is 31; Total loss is 3.8764979771205357\n",
            "Currrent Epoch is 32; Total loss is 3.8762786865234373\n",
            "Currrent Epoch is 33; Total loss is 3.876059831891741\n",
            "Currrent Epoch is 34; Total loss is 3.875841849190848\n",
            "Currrent Epoch is 35; Total loss is 3.8756251743861605\n",
            "Currrent Epoch is 36; Total loss is 3.8754098074776784\n",
            "Currrent Epoch is 37; Total loss is 3.8751940046037947\n",
            "Currrent Epoch is 38; Total loss is 3.874979509626116\n",
            "Currrent Epoch is 39; Total loss is 3.8747667585100447\n",
            "Currrent Epoch is 40; Total loss is 3.8745535714285713\n",
            "Currrent Epoch is 41; Total loss is 3.874341256277902\n",
            "Currrent Epoch is 42; Total loss is 3.8741302490234375\n",
            "Currrent Epoch is 43; Total loss is 3.873919241768973\n",
            "Currrent Epoch is 44; Total loss is 3.8737108503069195\n",
            "Currrent Epoch is 45; Total loss is 3.8735007149832588\n",
            "Currrent Epoch is 46; Total loss is 3.8732923235212056\n",
            "Currrent Epoch is 47; Total loss is 3.87308349609375\n",
            "Currrent Epoch is 48; Total loss is 3.8728768484933034\n",
            "Currrent Epoch is 49; Total loss is 3.8726693289620537\n",
            "Currrent Epoch is 50; Total loss is 3.8724639892578123\n",
            "Currrent Epoch is 51; Total loss is 3.8722586495535714\n",
            "Currrent Epoch is 52; Total loss is 3.8720546177455355\n",
            "Currrent Epoch is 53; Total loss is 3.8718505859375\n",
            "Currrent Epoch is 54; Total loss is 3.8716465541294642\n",
            "Currrent Epoch is 55; Total loss is 3.8714442661830355\n",
            "Currrent Epoch is 56; Total loss is 3.871241978236607\n",
            "Currrent Epoch is 57; Total loss is 3.871040562220982\n",
            "Currrent Epoch is 58; Total loss is 3.8708387102399553\n",
            "Currrent Epoch is 59; Total loss is 3.8706390380859377\n",
            "Currrent Epoch is 60; Total loss is 3.8704393659319196\n",
            "Currrent Epoch is 61; Total loss is 3.870239693777902\n",
            "Currrent Epoch is 62; Total loss is 3.8700408935546875\n",
            "Currrent Epoch is 63; Total loss is 3.8698416573660714\n",
            "Currrent Epoch is 64; Total loss is 3.8696446010044645\n",
            "Currrent Epoch is 65; Total loss is 3.86944580078125\n",
            "Currrent Epoch is 66; Total loss is 3.8692474365234375\n",
            "Currrent Epoch is 67; Total loss is 3.869051252092634\n",
            "Currrent Epoch is 68; Total loss is 3.868853323800223\n",
            "Currrent Epoch is 69; Total loss is 3.8686553955078127\n",
            "Currrent Epoch is 70; Total loss is 3.8684574672154017\n",
            "Currrent Epoch is 71; Total loss is 3.868260410853795\n",
            "Currrent Epoch is 72; Total loss is 3.8680607386997767\n",
            "Currrent Epoch is 73; Total loss is 3.8678628104073662\n",
            "Currrent Epoch is 74; Total loss is 3.867663138253348\n",
            "Currrent Epoch is 75; Total loss is 3.867462594168527\n",
            "Currrent Epoch is 76; Total loss is 3.867262486049107\n",
            "Currrent Epoch is 77; Total loss is 3.8670597621372766\n",
            "Currrent Epoch is 78; Total loss is 3.8668570382254464\n",
            "Currrent Epoch is 79; Total loss is 3.8666538783482145\n",
            "Currrent Epoch is 80; Total loss is 3.8664481026785715\n",
            "Currrent Epoch is 81; Total loss is 3.866241891043527\n",
            "Currrent Epoch is 82; Total loss is 3.866034371512277\n",
            "Currrent Epoch is 83; Total loss is 3.8658255440848213\n",
            "Currrent Epoch is 84; Total loss is 3.8656158447265625\n",
            "Currrent Epoch is 85; Total loss is 3.8654044015066966\n",
            "Currrent Epoch is 86; Total loss is 3.8651920863560267\n",
            "Currrent Epoch is 87; Total loss is 3.8649771554129466\n",
            "Currrent Epoch is 88; Total loss is 3.8647617885044645\n",
            "Currrent Epoch is 89; Total loss is 3.8645459856305804\n",
            "Currrent Epoch is 90; Total loss is 3.864328874860491\n",
            "Currrent Epoch is 91; Total loss is 3.8641087123325892\n",
            "Currrent Epoch is 92; Total loss is 3.8638902936662944\n",
            "Currrent Epoch is 93; Total loss is 3.863669695172991\n",
            "Currrent Epoch is 94; Total loss is 3.863447788783482\n",
            "Currrent Epoch is 95; Total loss is 3.863226318359375\n",
            "Currrent Epoch is 96; Total loss is 3.863004411969866\n",
            "Currrent Epoch is 97; Total loss is 3.8627820696149553\n",
            "Currrent Epoch is 98; Total loss is 3.8625597272600447\n",
            "Currrent Epoch is 99; Total loss is 3.862336948939732\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1MGLAOj5csm",
        "outputId": "a69f4bff-127a-4684-dc50-ff75d42dcb63"
      },
      "source": [
        "# Calculate test error\r\n",
        "test_epoch = 10\r\n",
        "total_test_loss = 0\r\n",
        "\r\n",
        "meta_variables = model.trainable_variables.copy()\r\n",
        "for k in range(len(meta_variables)):\r\n",
        "  meta_variables[k] = tf.constant(meta_variables[k])\r\n",
        "\r\n",
        "for j in range(n_test_task):\r\n",
        "  inner_optimizer = tf.optimizers.Adam(0.01)\r\n",
        "\r\n",
        "  # Update 20 times for the model to calculate test loss\r\n",
        "  for m in range(test_epoch):\r\n",
        "    with tf.GradientTape() as tape:\r\n",
        "      y_pred = model(x_test[j])\r\n",
        "      loss = tf.keras.losses.mean_squared_error(y_test[j], y_pred)\r\n",
        "\r\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\r\n",
        "    inner_optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n",
        "  \r\n",
        "  y_pred = model(x_test[j])\r\n",
        "  total_test_loss += tf.reduce_mean(tf.keras.losses.mean_squared_error(y_test[j], y_pred))\r\n",
        "\r\n",
        "  n_parameters = len(meta_variables)\r\n",
        "  for k in range(n_parameters):\r\n",
        "    # gradients[k] = tf.math.l2_normalize(gradients[k])\r\n",
        "    model.trainable_variables[k] = meta_variables[k]\r\n",
        "\r\n",
        "print(total_test_loss/30)\r\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(3.1326115, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXlNxRGY67vs",
        "outputId": "6a92c4c6-7c6e-4d4a-a38d-584508d20ad2"
      },
      "source": [
        "# Build the regression model\r\n",
        "model_2 = tf.keras.Sequential([tf.keras.Input(shape=(1, )),\r\n",
        "                             tf.keras.layers.Dense(40, activation='relu'),\r\n",
        "                             tf.keras.layers.Dense(40, activation='relu'),\r\n",
        "                             tf.keras.layers.Dense(1)\r\n",
        "])\r\n",
        "\r\n",
        "model_2_optimizer = tf.optimizers.Adam(0.01)\r\n",
        "# Calculate test error\r\n",
        "test_epoch = 10\r\n",
        "total_test_loss = 0\r\n",
        "\r\n",
        "meta_variables = model_2.trainable_variables.copy()\r\n",
        "for k in range(len(meta_variables)):\r\n",
        "  meta_variables[k] = tf.constant(meta_variables[k])\r\n",
        "\r\n",
        "for j in range(n_test_task):\r\n",
        "  \r\n",
        "  inner_optimizer = tf.optimizers.Adam(0.01)\r\n",
        "\r\n",
        "  # Update 20 times for the model to calculate test loss\r\n",
        "  for m in range(test_epoch):\r\n",
        "    with tf.GradientTape() as tape:\r\n",
        "      y_pred = model_2(x_test[j])\r\n",
        "      loss = tf.keras.losses.mean_squared_error(y_test[j], y_pred)\r\n",
        "\r\n",
        "    gradients = tape.gradient(loss, model_2.trainable_variables)\r\n",
        "    inner_optimizer.apply_gradients(zip(gradients, model_2.trainable_variables))\r\n",
        "  \r\n",
        "  y_pred = model_2(x_test[j])\r\n",
        "  total_test_loss += tf.reduce_mean(tf.keras.losses.mean_squared_error(y_test[j], y_pred))\r\n",
        "\r\n",
        "  n_parameters = len(meta_variables)\r\n",
        "  for k in range(n_parameters):\r\n",
        "    # gradients[k] = tf.math.l2_normalize(gradients[k])\r\n",
        "    model_2.trainable_variables[k] = meta_variables[k]\r\n",
        "\r\n",
        "print(total_test_loss/30)\r\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(3.277797, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}